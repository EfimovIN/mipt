{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seminar-01.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IeAqnCOOwFVW",
        "hUDOpKxHuQKS",
        "VYRLBwtvuQKi",
        "3xIsESNpuQKr",
        "_-nMGpEfuQKs",
        "UgRpAtU-uQKw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EfimovIN/mipt/blob/master/Seminar_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XkG_ZcvCuQJ0"
      },
      "source": [
        "# Семинар 1\n",
        "## Обучение с подкреплением: OpenAI gym, CrossEntropy Method (CEM), Deep CEM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_X7tCgzwFVJ",
        "colab_type": "text"
      },
      "source": [
        "## 1. OpenAI gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNooJhy2wFVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "\n",
        "#устанавливаем библиотеки для визуализации в colab\n",
        "!apt-get -qq -y install  libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay==0.2.5\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# запускаем virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
        "\n",
        "\n",
        "def show_animation(frames):\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    animate = lambda i: patch.set_data(frames[i])\n",
        "    ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "    return ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCULFSm0wFVS",
        "colab_type": "text"
      },
      "source": [
        "На семинаре мы будем пользоваться стандартными средами, реализованными в библиотеке OpenAI Gym (https://gym.openai.com)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFjwJBkfwFVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "0faa087e-40cf-46e4-ac63-f2074bdcdebf"
      },
      "source": [
        "import gym\n",
        "\n",
        "# создаем окружение\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env.reset()\n",
        "# рисуем картинку\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EwOzYf-wFVU",
        "colab_type": "text"
      },
      "source": [
        "Основные методы класса Env:\n",
        "\n",
        "* reset() - инициализация окружения, возвращает первое наблюдение (состояние)\n",
        "* render() - визуализация текущего состояния среды\n",
        "* step($a$) - выполнить в среде действие a и получить кортеж: $<s_{t+1}, r_t, done, info>$, где done флаг заверешния, а info - дополнительная информация.\n",
        "\n",
        "Прежде чем начать взаимодействие с окружением, нужно использовать метод reset():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbVd_yAwwFVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "423492f5-339f-4378-e678-5ff981f106e5"
      },
      "source": [
        "state0 = env.reset()\n",
        "print(\"изначальное состояние среды:\", state0)\n",
        "# выполняем действие 2 \n",
        "new_state, reward, done, _ = env.step(2)\n",
        "print(\"новое состояние:\", new_state)\n",
        "print(\"вознаграждение\", reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "изначальное состояние среды: [-0.52959528  0.        ]\n",
            "новое состояние: [-0.52855031  0.00104497]\n",
            "вознаграждение -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeAqnCOOwFVW",
        "colab_type": "text"
      },
      "source": [
        "### Среда MountainCar-v0\n",
        "* Состояния: Type: Box(2)\n",
        "\n",
        "\n",
        "\n",
        "Num | Observation  | Min  | Max  \n",
        "----|--------------|------|----   \n",
        "0   | position     | -1.2 | 0.6\n",
        "1   | velocity     | -0.07| 0.07\n",
        "\n",
        "\n",
        "* Действия: Type: Discrete(3)\n",
        "\n",
        "\n",
        "\n",
        "Num | Action|\n",
        "----|-------------|\n",
        "0   | push left   |\n",
        "1   | no push     |\n",
        "2   | push right  |\n",
        "\n",
        "* Вознаграждения: -1 за каждый шаг, пока не достигнута цель \n",
        "\n",
        "* Начальное состояние: Случайная позиция от -0.6 до -0.4 с нулевой скоростью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViuSxNwYwFVX",
        "colab_type": "text"
      },
      "source": [
        "### Задание\n",
        "В среде MountainCar-v0 мы хотим, чтобы тележка достигла флага. Давайте решим эту задачу, не используя обучение с подкреплением. Модифицируйте код функции act() ниже для выполнения этого задания. Функция получает на вход состояние среды и должна вернуть действие. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aok-RLYowFVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def act(s):\n",
        "    actions = {'left': 0, 'stop': 1, 'right': 2}\n",
        "    \n",
        "    # пример: можем попробовать ехать только влево\n",
        "    # action = actions['left'] \n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQkZdmtiwFVZ",
        "colab_type": "text"
      },
      "source": [
        "Проверяем решение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm6jwUThwFVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# создаем окружение, с ограничением на число шагов, используя wrapper (обертку) TimeLimit\n",
        "env = gym.wrappers.TimeLimit(gym.make(\"MountainCar-v0\"), max_episode_steps=250)\n",
        "# проводим инициализацию и запоминаем начальное состояние\n",
        "s = env.reset()\n",
        "done = False\n",
        "\n",
        "frames = []\n",
        "\n",
        "frames.append(env.render(mode = 'rgb_array'))\n",
        "\n",
        "while not done:\n",
        "    # выполняем действие, получаем s, r, done\n",
        "    s, r, done, _ = env.step(act(s))\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # визуализируем окружение\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "if s[0] > 0.47:\n",
        "    print(\"Задание выполнено!\")\n",
        "else:\n",
        "    print(\"\"\"Исправьте функцию выбора действия!\"\"\")\n",
        "\n",
        "HTML(show_animation(frames).to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UM7vcYORuQJ1"
      },
      "source": [
        "## 2. Crossentropy Method\n",
        "\n",
        "В этом пункте мы посмотрим на то, как решить задачи RL с помощью метода crossentropy.\n",
        "\n",
        "Рассмотрим пример с задачей Taxi [Dietterich, 2000]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i5yU3RZuuQJ2",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xyi1DDjQuQJ8",
        "colab": {}
      },
      "source": [
        "n_states  = env.observation_space.n\n",
        "n_actions = env.action_space.n  \n",
        "\n",
        "print(f\"состояний: {n_states} действий: {n_actions}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hR-L-rHquQKA"
      },
      "source": [
        "В этот раз нашей стратегией будет вероятностное распределение. \n",
        "\n",
        "$\\pi(s,a) = P(a|s)$\n",
        "\n",
        "Для задачи такси мы можем использовать таблицу: \n",
        "\n",
        "policy[s,a] = P(выбрать действие a | в состоянии s)\n",
        "\n",
        "Создадим \"равномерную\" стратегию в виде двумерного массива с \n",
        "равномерным распределением по действиям и сгенерируем игровую сессию с такой стратегией"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sVBlXo-huQKB",
        "colab": {}
      },
      "source": [
        "def initialize_policy(n_states, n_actions):\n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    return policy\n",
        "\n",
        "policy = initialize_policy(n_states, n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-xqVJHT4uQKE",
        "colab": {}
      },
      "source": [
        "assert type(policy) in (np.ndarray, np.matrix)\n",
        "assert np.allclose(policy, 1./n_actions)\n",
        "assert np.allclose(np.sum(policy, axis=1), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDx_ek7vuQKH"
      },
      "source": [
        "### Генерация сессий взаимодейтсвия со средой.\n",
        "\n",
        "Мы будем запоминать все состояния, действия и вознаграждения за эпизод."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QlsAeF7EuQKI",
        "colab": {}
      },
      "source": [
        "def generate_session(env, policy, t_max=10**4):\n",
        "    \"\"\"\n",
        "    Игра идет до конца эпизода или до t_max шагов в окружении. \n",
        "    :param policy: [n_states,n_actions] \n",
        "    :returns: states - список состояний, actions - список действий, total_reward - итоговое вознаграждение\n",
        "    \"\"\"\n",
        "    states, actions = [], []\n",
        "    total_reward = 0.\n",
        "\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # Выбираем действие согласно текущей стратегии\n",
        "        # Подсказка: можно воспользоваться np.random.choice для сэмплирования действий\n",
        "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
        "        # a = \n",
        "        ####### Здесь ваш код ########## \n",
        "        raise NotImplementedError \n",
        "        ################################\n",
        "\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # Запоминаем нужную нам информацию\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        total_reward += r\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HM2-QI3UuQKL",
        "colab": {}
      },
      "source": [
        "s, a, r = generate_session(env, policy)\n",
        "assert type(s) == type(a) == list\n",
        "assert len(s) == len(a)\n",
        "assert type(r) in [float, np.float]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWqyUSLJuQKP",
        "colab": {}
      },
      "source": [
        "# посмотрим на изначальное распределение вознаграждения\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n",
        "\n",
        "plt.hist(sample_rewards, bins=20)\n",
        "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
        "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hUDOpKxHuQKS"
      },
      "source": [
        "### Реализация метода crossentropy  \n",
        "\n",
        "Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FGIweAi5uQKT",
        "colab": {}
      },
      "source": [
        "def select_elites(states_batch, actions_batch, \n",
        "                  rewards_batch, percentile=50):\n",
        "    \"\"\"\n",
        "    Выбирает состояния и действия с заданным перцентилем (rewards >= percentile)\n",
        "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
        "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
        "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
        "    \n",
        "    :returns: elite_states, elite_actions - одномерные \n",
        "    списки состояния и действия, выбранных сессий\n",
        "    \"\"\"\n",
        "    # нужно найти порог вознаграждения по процентилю\n",
        "    # reward_threshold =\n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    \n",
        "    # в соответствии с найденным порогом - отобрать \n",
        "    # подходящие состояния и действия\n",
        "    # elite_states = \n",
        "    # elite_actions = \n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    return elite_states, elite_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q34NZBHHuQKW",
        "colab": {}
      },
      "source": [
        "states_batch = [\n",
        "    [1, 2, 3],     # game1\n",
        "    [4, 2, 0, 2],  # game2\n",
        "    [3, 1],        # game3\n",
        "]\n",
        "\n",
        "actions_batch = [\n",
        "    [0, 2, 4],     # game1\n",
        "    [3, 2, 0, 1],  # game2\n",
        "    [3, 3],        # game3\n",
        "]\n",
        "rewards_batch = [\n",
        "    3,  # game1\n",
        "    4,  # game2\n",
        "    5,  # game3\n",
        "]\n",
        "\n",
        "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
        "test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
        "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
        "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
        "\n",
        "assert np.all(\n",
        "    test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) \\\n",
        "       and np.all(\n",
        "    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n",
        "    \"Для процентиля 0 необходимо выбрать все состояния \" \\\n",
        "    \"и действия в хронологическом порядке\"\n",
        "\n",
        "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1])\\\n",
        "   and np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]), \\\n",
        "    \"Для процентиля 30 необходимо выбрать \" \\\n",
        "    \"состояния/действия из [3:]\"\n",
        "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
        "       np.all(test_result_90[1] == [3, 3]), \\\n",
        "    \"Для процентиля 90 необходимо выбрать состояния \" \\\n",
        "    \"действия одной игры\"\n",
        "assert np.all(test_result_100[0] == [3, 1]) and \\\n",
        "       np.all(test_result_100[1] == [3, 3]), \\\n",
        "    \"Проверьте использование знаков: >=,  >. \" \\\n",
        "    \"Также проверьте расчет процентиля\"\n",
        "print(\"Тесты пройдены!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZZzLzP7PuQKb"
      },
      "source": [
        "Теперь мы хотим написать обновляющуюся стратегию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PecfT_xEuQKc",
        "colab": {}
      },
      "source": [
        "def update_policy(elite_states,elite_actions):\n",
        "    \"\"\"\n",
        "    Новой стратегией будет:\n",
        "    policy[s_i,a_i] ~ #[вхождения  si/ai в лучшие states/actions]\n",
        "    \n",
        "    Не забудьте про нормализацию состояний.\n",
        "    Если какое-то состояние не было посещено, \n",
        "    то используйте равномерное распределение 1./n_actions\n",
        "    \n",
        "    :param elite_states:  список состояний\n",
        "    :param elite_actions: список действий\n",
        "    \"\"\"\n",
        "    new_policy = np.zeros([n_states,n_actions])\n",
        "    for state in range(n_states):\n",
        "        # обновляем стратегию - нормируем новые частоты \n",
        "        # действий и не забываем про непосещенные состояния\n",
        "        # new_policy[state, a] =         \n",
        "        ####### Здесь ваш код ##########\n",
        "        raise NotImplementedError\n",
        "        ################################\n",
        "        \n",
        "    return new_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DsCFXHMRuQKf",
        "colab": {}
      },
      "source": [
        "elite_states, elite_actions = (\n",
        "    [1, 2, 3, 4, 2, 0, 2, 3, 1],\n",
        "    [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
        "\n",
        "new_policy = update_policy(elite_states, elite_actions)\n",
        "\n",
        "assert np.isfinite(\n",
        "    new_policy).all(), \"Стратегия не должна содержать \" \\\n",
        "                       \"NaNs или +-inf. Проверьте \" \\\n",
        "                       \"деление на ноль. \"\n",
        "assert np.all(\n",
        "    new_policy >= 0), \"Стратегия не должна содержать \" \\\n",
        "                      \"отрицательных вероятностей \"\n",
        "assert np.allclose(new_policy.sum(axis=-1),\n",
        "                   1), \"Суммарная\\ вероятность действий\"\\\n",
        "                       \"для состояния должна равняться 1\"\n",
        "reference_answer = np.array([\n",
        "    [1., 0., 0., 0., 0.],\n",
        "    [0.5, 0., 0., 0.5, 0.],\n",
        "    [0., 0.33333333, 0.66666667, 0., 0.],\n",
        "    [0., 0., 0., 0.5, 0.5]])\n",
        "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
        "print(\"Тесты пройдены!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VYRLBwtvuQKi"
      },
      "source": [
        "### Цикл обучения\n",
        "\n",
        "Визуализириуем наш процесс обучения и также будем измерять распределение получаемых за сессию вознаграждений "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tmBnonZCuQKj",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
        "    \"\"\"\n",
        "    Удобная функция, для визуализации результатов.\n",
        "    \"\"\"\n",
        "\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    threshold = np.percentile(rewards_batch, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "    \n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(rewards_batch, range=reward_range)\n",
        "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
        "               [0], [100], label=\"percentile\", color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GXhGLvjPuQKm",
        "colab": {}
      },
      "source": [
        "# инициализируем стратегию\n",
        "policy = initialize_policy(n_states, n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vYncUOBfuQKo",
        "colab": {}
      },
      "source": [
        "n_sessions = 250  # количество сессий для сэмплирования\n",
        "percentile = 50  # перцентиль \n",
        "learning_rate = 0.5 # то как быстро стратегия будет обновляться \n",
        "\n",
        "log = []\n",
        "\n",
        "for i in range(100):\n",
        "    # генерируем n_sessions сессий\n",
        "    # %time sessions = []\n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
        "    # отбираем лучшие действия и состояния ###\n",
        "    # elite_states, elite_actions = \n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    # обновляем стратегию\n",
        "    # new_policy =\n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    policy = learning_rate * new_policy + (1 - learning_rate) * policy\n",
        "\n",
        "    # display results on chart\n",
        "    show_progress(rewards_batch, log, percentile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3xIsESNpuQKr"
      },
      "source": [
        "### Посмотрим на результаты\n",
        "Задача такси быстро сходится, начиня с вознаграждения -1000 к почти оптимальному значению, а потом опять падает до -50/-100. Это вызвано случайностью в самом окружении - случайное начальное состояние пассажира и такси, в начале каждого эпизода. \n",
        "\n",
        "В случае если алгоритм CEM не сможет научиться тому, как решить задачу из какого-то стартового положения, он просто отбросит этот эпизод, т.к. не будет сессий, которые переведут этот эпизод в топ лучших. \n",
        "\n",
        "Для решения этой проблемы можно уменьшить threshold (порог лучших состояний) или изменить способ оценки стратегии, используя новую стратегию, полученную из каждого начального состояния и действия (теоретически правильный способ)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_-nMGpEfuQKs"
      },
      "source": [
        "## 3. Deep CEM\n",
        "\n",
        "В данной части мы рассмотрим применение CEM вместе с нейронной сетью.\n",
        "Будем обучать многослойную нейронную сеть для решения простой задачи с непрерывным пространством действий.\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/8f39c7f54a7798e7f80c9ec5c0bb610696e5c5b7/68747470733a2f2f7469702e64756b652e6564752f696e646570656e64656e745f6c6561726e696e672f677265656b2f6c6573736f6e2f64696767696e675f6465657065725f66696e616c2e6a7067\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sWFuA1hWuQKs"
      },
      "source": [
        "Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным пространством состояний.\n",
        "https://gym.openai.com/envs/CartPole-v0/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ZhZahUGuQKt",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "                                        \n",
        "print(f\"состояний: {state_dim} действий: {n_actions}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UgRpAtU-uQKw"
      },
      "source": [
        "### Стратегия с нейронной сетью\n",
        "\n",
        "Попробуем заменить метод обновления вероятностей на нейронную сеть. \n",
        "Будем пользоваться упрощенной реализацией нейронной сети из пакета Scikit-learn.\n",
        "Нам потребуется: \n",
        "* agent.partial_fit(states, actions) - делает один проход обучения по данным. Максимизирует вероятность :actions: из :states:\n",
        "* agent.predict_proba(states) - предсказыает вероятность каждого из действий, в виде матрицы размера [len(states), n_actions]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5fOPMKcYuQKx",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "agent = MLPClassifier(\n",
        "    hidden_layer_sizes=(20, 20),\n",
        "    activation='tanh',\n",
        ")\n",
        "\n",
        "# initialize agent to the dimension of state space and number of actions\n",
        "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9W-5nV3uQK0",
        "colab": {}
      },
      "source": [
        "def generate_session(env, agent, t_max=1000):\n",
        "    \n",
        "    states,actions = [],[]\n",
        "    total_reward = 0\n",
        "    \n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        # предсказываем вероятности действий по сети и \n",
        "        # выбираем одно действие\n",
        "        # probs = \n",
        "        # a = \n",
        "        ####### Здесь ваш код ##########\n",
        "        raise NotImplementedError\n",
        "        ################################\n",
        "        \n",
        "        new_s,r,done,info = env.step(a)\n",
        "        \n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        total_reward+=r\n",
        "        \n",
        "        s = new_s\n",
        "        if done: break\n",
        "    return states,actions,total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0pntOSnguQK2",
        "colab": {}
      },
      "source": [
        "dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n",
        "print(\"состояния:\", np.stack(dummy_states))\n",
        "print(\"действия:\", dummy_actions)\n",
        "print(\"вознаграждение:\", dummy_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HVDwmDFAuQK4",
        "colab": {}
      },
      "source": [
        "n_sessions = 100\n",
        "percentile = 70\n",
        "log = []\n",
        "\n",
        "for i in range(100):\n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
        "    \n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    # обновляем стратегию, для предсказания лучших состояний\n",
        "    # elite_actions(y) из elite_states(X)\n",
        "    ####### Здесь ваш код ##########\n",
        "    raise NotImplementedError\n",
        "    ################################\n",
        "    \n",
        "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
        "\n",
        "    if np.mean(rewards_batch) > 190:\n",
        "        print(\"Принято!\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atjeEor1wFWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}